{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**14장 – 합성곱 신경망을 사용한 컴퓨터 비전**"
      ],
      "metadata": {
        "id": "RzSFNc8n7dfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다양한 CNN 모델"
      ],
      "metadata": {
        "id": "SifvLyZ69lsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ILSVRC은 ImageNet Large Scale Visual Recognition Challenge의 약자로 이미지 인식(image recognition) 경진대회이다. 여기서 이미지 인식과 이미지 분류(image classification)는 같은 의미를 갖는다. 대용량의 이미지셋을 주고 이미지 분류 알고리즘의 성능을 평가하는 대회이다. 2010년에 시작되었다. 이 대회에서 우승한 알고리즘들이 컴퓨터 비전 분야 발전에 큰 역할을 해왔다. 이른바 대세 알고리즘들이 되었다.  \n",
        "\n"
      ],
      "metadata": {
        "id": "6N6qyQmj9rIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2010년, 2011년에 우승을 차지한 알고리즘들은 얕은 구조(shallow architecture)를 가진 것들이었다. 얕은 구조를 가진 알고리즘에서는 이미지 인식에 유용할만한 특성들을 개발자들이 임의로 결정해서 도출했다. 그러나 2012년 CNN 기반 딥러닝 알고리즘 AlexNet이 우승을 차지한 이후에는 깊은 구조(deep architecture)를 가진 알고리즘들이 우승을 차지했다. AlexNet은 약 26%였던 인식 오류율을 16%까지 낮췄다. 10%를 낮춘다는 것은 상상하기도 힘든 일이었다. 얕은 구조 기반 알고리즘들로는 0.1% 낮추는 것도 쉽지 않았기 때문이다. "
      ],
      "metadata": {
        "id": "wnHuwz4e9tkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 기반 알고리즘이 사용되면서 인식 오류는 확실하게 낮아졌다. 심지어 2015년에는 사람의 정확도라고 알려진 5%를 추월했다. 2017년의 SENet의 경우 2.3%로 사람의 인식 에러율의 절반도 안된다. "
      ],
      "metadata": {
        "id": "lyNkooXW9zs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/ILSVRC_error_rate.jpg\"/>"
      ],
      "metadata": {
        "id": "RkkcBL8T91_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지넷 데이터셋은 1,000개의 클래스로 구성되며 총 백만 개가 넘는 데이터를 포함한다. 약 120만 개는 학습(training)에 쓰고, 5만개는 검증(validation)에 사용합니다. 학습 데이터셋 용량은 약 138GB, 검증 데이터셋 용량은 약 6GB입니다. 특히 분류(classification) 문제에 관심이 있는 딥러닝 연구자라면 대개 이미지넷 데이터셋을 다운로드하는 편입니다. 학습 데이터를 확인해 보면 각 클래스당 약 1,000개가량의 사진으로 구성되어 있습니다.\n",
        "https://www.image-net.org/\n",
        "\n",
        "\n",
        "이미지넷(ImageNet) 데이터셋은 MNIST, CIFAR 데이터셋과 더불어 굉장히 유명한 데이터셋이다. 일반적으로 MNIST나 CIFAR는 아이디어에 대한 검증 목적으로 사용합니다. 최신 컴퓨팅 장치를 기준으로 보았을 때 데이터셋 자체가 아주 작기 때문에, 어지간한 크기의 뉴럴 네트워크를 이용해도 학습 과정에서 길어야 하루 이내의 시간이 소요되기 때문입니다.\n",
        "\n",
        "\n",
        "- MNIST: 0부터 9까지의 28 x 28 손글씨 사진을 모은 데이터셋 (학습용: 60,000개 / 테스트용: 10,000)\n",
        "\n",
        "- CIFAR-10: 10개의 클래스로 구분된 32 x 32 사물 사진을 모은 데이터셋 (학습용: 50,000개 / 테스트용: 10,000개); https://www.cs.toronto.edu/~kriz/cifar.html -- 각각의 클래스는 6000개의 이미지로 이루어짐. 또 5000개는 학습 데이터, 1000개는 테스트 데이터입니다.\n",
        "\n",
        "- CIFAR-100: 100개의 클래스로 구분된 32 x 32 사물 사진을 모은 데이터셋 (학습용: 50,000개 / 테스트용: 10,000개); https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "   -- 각각의 클래슨느 600개의 이미지로 이루어 있으며, 100개의 클랙스는 20개의 슈퍼클래스로 분류. 또, 500개는 학습데이터, 100개는 테스트용 데이터입니다.\n"
      ],
      "metadata": {
        "id": "JkPG3IT7QMJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5, AlexNet, GoogLeNet, VGGNet, ResNet, SeNet에 대한 모델들을 알아 보겠습니다."
      ],
      "metadata": {
        "id": "0NAvXDXeBte4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet-5"
      ],
      "metadata": {
        "id": "909pDNA_Dl0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet은 CNN을 처음으로 개발한 얀 르쿤(Yann Lecun) 연구팀이 1998년에 개발한 CNN 알고리즘의 이름이다. original 논문 제목은 \"Gradient-based learning applied to document recognition\"이다. 우선 LeNet-5의 구조를 살펴보자 (LeCunn 1998; http://yann.lecun.com/exdb/lenet/index.html) "
      ],
      "metadata": {
        "id": "GLrOK7GPFmqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/LeNet-5.jpg\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "AXGPYZg1Dn7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그림에서 볼 수 있듯이 LeNet-5는 인풋, 3개의 컨볼루션 레이어(C1, C3, C5), 2개의 서브샘플링 레이어(S2, S4), 1층의 full-connected 레이어(F6), 아웃풋 레이어로 구성되어 있다. 참고로 C1부터 F6까지 활성화 함수로 tanh을 사용한다.   \n",
        "\n",
        "\n",
        "\n",
        "1) C1 레이어: 입력 영상(여기서는 32 x 32 사이즈의 이미지)을 6개의 5 x 5 필터와 컨볼루션 연산을 해준다. 그 결과 6장의 28 x 28 특성 맵을 얻게 된다. \n",
        "\n",
        "\n",
        "\n",
        "- 훈련해야할 파라미터 개수: (가중치*입력맵개수 + 바이어스)*특성맵개수 = (5*5*1 + 1)*6 = 156\n",
        "\n",
        "\n",
        "\n",
        "2) S2 레이어: 6장의 28 x 28 특성 맵에 대해 서브샘플링을 진행한다. 결과적으로 28 x 28 사이즈의 특성 맵이 14 x 14 사이즈의 특성맵으로 축소된다. 2 x 2 필터를 stride 2로 설정해서 서브샘플링해주기 때문이다. 사용하는 서브샘플링 방법은 평균 풀링(average pooling)이다. \n",
        "\n",
        "\n",
        "\n",
        "- 훈련해야할 파라미터 개수: (가중치 + 바이어스)*특성맵개수 = (1 + 1)*6 = 12\n",
        "\n",
        "\n",
        "\n",
        "평균풀링인데 왜 훈련해야할 파라미터가 필요한지 의아할 수 있는데, original 논문에 의하면 평균을 낸 후에 한 개의 훈련가능한 가중치(trainable weight)를 곱해주고 또 한 개의 훈련가능한 바이어스(trainable bias)를 더해준다고 한다. 그 값이 시그모이드 함수를 통해 활성화된다. 참고로 그 가중치와 바이어스는 시그모이드의 비활성도를 조절해준다고 한다. \n",
        "\n",
        "\n",
        "3) C3 레이어: 6장의 14 x 14 특성맵에 컨볼루션 연산을 수행해서 16장의 10 x 10 특성맵을 산출해낸다. \n",
        "\n",
        "4) S4 레이어: 16장의 10 x 10 특성 맵에 대해서 서브샘플링을 진행해 16장의 5 x 5 특성 맵으로 축소시킨다. \n",
        "- 훈련해야할 파라미터 개수: (가중치 + 바이어스)*특성맵개수 = (1 + 1)*16 = 32\n",
        "\n",
        "5) C5 레이어: 16장의 5 x 5 특성맵을 120개 5 x 5 x 16 사이즈의 필터와 컨볼루션 해준다. 결과적으로 120개 1 x 1 특성맵이 산출된다. \n",
        "- 훈련해야할 파라미터 개수: (가중치*입력맵개수 + 바이어스)*특성맵 개수 = (5*5*16 + 1)*120 = 48120\n",
        "\n",
        "\n",
        "6) F6 레이어: 84개의 유닛을 가진 피드포워드 신경망이다. C5의 결과를 84개의 유닛에 연결시킨다. \n",
        "\n",
        "\n",
        "- 훈련해야할 파라미터 개수: 연결개수 = (입력개수 + 바이어스)*출력개수 = (120 + 1)*84 = 10164\n",
        "\n",
        "\n",
        "\n",
        "7) 아웃풋 레이어: 10개의 Euclidean radial basis function(RBF) 유닛들로 구성되어있다. 각각 F6의 84개 유닛으로부터 인풋을 받는다. 최종적으로 이미지가 속한 클래스를 알려준다. \n",
        "\n",
        "\n",
        "\n",
        "LeNet-5를 제대로 가동하기 위해 훈련해야할 파라미터는 총 156 + 12 + 1516 + 32 + 48120 + 10164 = 60000개다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C4gIA5ImFuGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation, Flatten, Dense\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import mnist # dataset은 mnist를 이용한 데이터셋이다."
      ],
      "metadata": {
        "id": "2AN1__ISGiXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x_ori, train_y_ori), (test_x_ori, test_y_ori) = mnist.load_data()\n",
        "plt.imshow(train_x_ori[1], cmap='Greys')\n",
        "plt.imshow(train_x_ori[5], cmap='Greys')\n",
        "\n",
        "train_x_ori = train_x_ori.astype('float32')\n",
        "test_x_ori = test_x_ori.astype('float32')"
      ],
      "metadata": {
        "id": "ivm4Ev5pGu3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x_ori / 255\n",
        "test_x = test_x_ori / 255\n",
        "train_x = train_x[:, :, :,np.newaxis,]\n",
        "test_x = test_x[:, :, :, np.newaxis]"
      ],
      "metadata": {
        "id": "Lw6hTw9LGy-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = np_utils.to_categorical(train_y_ori, 10)\n",
        "test_y = np_utils.to_categorical(test_y_ori, 10)"
      ],
      "metadata": {
        "id": "S8YJqciAG1lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet : \n",
        "    @staticmethod\n",
        "    def build(input_shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(20, kernel_size = 5, padding=\"same\", input_shape=input_shape))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
        "        model.add(Conv2D(50, kernel_size = 5, padding=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(500))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(10))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model\n",
        "\n",
        "model = LeNet.build(input_shape = (28, 28, 1), classes = 10)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "E_IDePNaG411"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
        "history = model.fit(train_x, train_y, batch_size=256, epochs=10, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "ID-Nldp7HoNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_x, test_y, verbose=1)\n",
        "print(\"최종 정확도 : \" + str( score[1] * 100 ) + \" %\" )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "bnPAr3e3G-9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlexNet"
      ],
      "metadata": {
        "id": "2PvDeNAmB2xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlexNet은 2012년에 개최된 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회의 우승을 차지한 컨볼루션 신경망(CNN) 구조이다. CNN의 부흥에 아주 큰 역할을 한 구조라고 말할 수 있다. AlexNet의 original 논문명은 \"ImageNet Classification with Deep Convolutional Neural Networks\"이다. 이 논문의 첫번째 저자가 Alex Khrizevsky이기 때문에 그의 이름을 따서 AlexNet이라고 부른다. CNN의 가장 간단한 구조 중의 하나인 LeNet-5와 비교하면서 설명하니, LeNet-5 포스팅을 먼저 읽고 시작하시는 것을 추천합니다. \n",
        "\n"
      ],
      "metadata": {
        "id": "OqcKrjRJB4Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlexNet의 기본구조는 LeNet-5(LeCunn 1998; http://yann.lecun.com/exdb/lenet/index.html) 와 크게 다르지 않다. 2개의 GPU로 병렬연산을 수행하기 위해서 병렬적인 구조로 설계되었다는 점이 가장 큰 변화이다. AlexNet의 구조도를 살펴보자.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/AlexNet.png\"/>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "PDdtnsNgB66g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlexNet은 8개의 레이어로 구성되어 있다. 5개의 컨볼루션 레이어와 3개의 full-connected 레이어로 구성되어 있다. 두번째, 네번째, 다섯번째 컨볼루션 레이어들은 전 단계의 같은 채널의 특성맵들과만 연결되어 있는 반면, 세번째 컨볼루션 레이어는 전 단계의 두 채널의 특성맵들과 모두 연결되어 있다는 것을 집고 넘어가자. \n",
        "\n",
        " \n",
        "\n",
        "이제 각 레이어마다 어떤 작업이 수행되는지 살펴보자. 우선 AlexNet에 입력 되는 것은 227 x 227 x 3 이미지다. (227 x 227 사이즈의 RGB 컬러 이미지를 뜻한다.) 그림에는 224로 되어 있는데 잘못된 겁니다. \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "1) 첫번째 레이어(컨볼루션 레이어): 96개의 11 x 11 x 3 사이즈 필터커널로 입력 영상을 컨볼루션해준다. 컨볼루션 보폭(stride)를 4로 설정했고, zero-padding은 사용하지 않았다. zero-padding은 컨볼루션으로 인해 특성맵의 사이즈가 축소되는 것을 방지하기 위해, 또는 축소되는 정도를 줄이기 위해 영상의 가장자리 부분에 0을 추가하는 것이다. 결과적으로 55 x 55 x 96 특성맵(96장의 55 x 55 사이즈 특성맵들)이 산출된다. 그 다음에 ReLU 함수로 활성화해준다. 이어서 3 x 3 overlapping max pooling이 stride 2로 시행된다. 그 결과 27 x 27 x 96 특성맵을 갖게 된다. 그 다음에는 수렴 속도를 높이기 위해 local response normalization이 시행된다. local response normalization은 특성맵의 차원을 변화시키지 않으므로, 특성맵의 크기는 27 x 27 x 96으로 유지된다. \n",
        "\n",
        " \n",
        "\n",
        "* 컨볼루션 이후 특성맵의 사이즈가 어떻게 결정되는지에 대해서 의문을 가질 분들이 계실 것 같습니다. 그래서 이에 대한 내용을 AlexNet보다 이후에 나온 모델들인 VGG-F, VGG-M, VGG-S을 설명하면서 하단부에 소개했으니 참고하시기 바랍니다(https://bskyvision.com/420)\n",
        "\n",
        " \n",
        "\n",
        "2) 두번째 레이어(컨볼루션 레이어): 256개의 5 x 5 x 48 커널을 사용하여 전 단계의 특성맵을 컨볼루션해준다. stride는 1로, zero-padding은 2로 설정했다. 따라서 27 x 27 x 256 특성맵(256장의 27 x 27 사이즈 특성맵들)을 얻게 된다. 역시 ReLU 함수로 활성화한다. 그 다음에 3 x 3 overlapping max pooling을 stride 2로 시행한다. 그 결과 13 x 13 x 256 특성맵을 얻게 된다. 그 후 local response normalization이 시행되고, 특성맵의 크기는 13 x 13 x 256으로 그대로 유지된다. \n",
        "\n",
        " \n",
        "\n",
        "3) 세번째 레이어(컨볼루션 레이어): 384개의 3 x 3 x 256 커널을 사용하여 전 단계의 특성맵을 컨볼루션해준다. stride와 zero-padding 모두 1로 설정한다. 따라서 13 x 13 x 384 특성맵(384장의 13 x 13 사이즈 특성맵들)을 얻게 된다. 역시 ReLU 함수로 활성화한다. \n",
        "\n",
        " \n",
        "\n",
        "4) 네번째 레이어(컨볼루션 레이어): 384개의 3 x 3 x 192 커널을 사용해서 전 단계의 특성맵을 컨볼루션해준다. stride와 zero-padding 모두 1로 설정한다. 따라서 13 x 13 x 384 특성맵(384장의 13 x 13 사이즈 특성맵들)을 얻게 된다. 역시 ReLU 함수로 활성화한다. \n",
        "\n",
        " \n",
        "\n",
        "5) 다섯번째 레이어(컨볼루션 레이어): 256개의 3 x 3 x 192 커널을 사용해서 전 단계의 특성맵을 컨볼루션해준다. stride와 zero-padding 모두 1로 설정한다. 따라서 13 x 13 x 256 특성맵(256장의 13 x 13 사이즈 특성맵들)을 얻게 된다. 역시 ReLU 함수로 활성화한다. 그 다음에 3 x 3 overlapping max pooling을 stride 2로 시행한다. 그 결과 6 x 6 x 256 특성맵을 얻게 된다. \n",
        "\n",
        " \n",
        "\n",
        "6) 여섯번째 레이어(Fully connected layer): 6 x 6 x 256 특성맵을 flatten해줘서 6 x 6 x 256 = 9216차원의 벡터로 만들어준다. 그것을 여섯번째 레이어의 4096개의 뉴런과 fully connected 해준다. 그 결과를 ReLU 함수로 활성화한다. \n",
        "\n",
        " \n",
        "\n",
        "7) 일곱번째 레이어(Fully connected layer): 4096개의 뉴런으로 구성되어 있다. 전 단계의 4096개 뉴런과 fully connected되어 있다. 출력 값은 ReLU 함수로 활성화된다. \n",
        "\n",
        " \n",
        "\n",
        "8) 여덟번째 레이어(Fully connected layer): 1000개의 뉴런으로 구성되어 있다. 전 단계의 4096개 뉴런과 fully connected되어 있다. 1000개 뉴런의 출력값에 softmax 함수를 적용해 1000개 클래스 각각에 속할 확률을 나타낸다. \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "총, 약 6천만개의 파라미터가 훈련되어야 한다. LeNet-5에서 6만개의 파라미터가 훈련되야했던 것과 비교하면 천배나 많아졌다. 하지만 그만큼 컴퓨팅 기술도 좋아졌고, 훈련시간을 줄이기 위한 방법들도 사용되었기 때문에 훈련이 가능했다. 예전 기술 같으면 상상도 못할 연산량이다. \n",
        "\n"
      ],
      "metadata": {
        "id": "Tl5Q58UdCCqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "eW7NqqOLVGM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "NQp2neKGVJIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds=tf.data.Dataset.from_tensor_slices((train_images,train_labels))\n",
        "test_ds=tf.data.Dataset.from_tensor_slices((test_images,test_labels))"
      ],
      "metadata": {
        "id": "eyn3gxShVK-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30,30))\n",
        "for i,(image,label) in enumerate(train_ds.take(20)):\n",
        "    #print(label)\n",
        "    ax=plt.subplot(5,5,i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(CLASS_NAMES[label.numpy()[0]])\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "0N9P-JAOVMue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image,label):\n",
        "    image=tf.image.per_image_standardization(image)\n",
        "    image=tf.image.resize(image,(64,64))\n",
        "    \n",
        "    return image,label"
      ],
      "metadata": {
        "id": "8U9TjNHSbK2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_size=tf.data.experimental.cardinality(train_ds).numpy()\n",
        "test_ds_size=tf.data.experimental.cardinality(test_ds).numpy()\n",
        "print('Train size:',train_ds_size)\n",
        "print('Test size:',test_ds_size)"
      ],
      "metadata": {
        "id": "1WVvxrijXElI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds=(train_ds\n",
        "          .map(process_image)\n",
        "          .shuffle(buffer_size=train_ds_size)\n",
        "          .batch(batch_size=32,drop_remainder=True)\n",
        "         )\n",
        "test_ds=(test_ds\n",
        "          .map(process_image)\n",
        "          .shuffle(buffer_size=test_ds_size)\n",
        "          .batch(batch_size=32,drop_remainder=True)\n",
        "         )"
      ],
      "metadata": {
        "id": "VUm15-JVbOnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(64,64,3)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1024,activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1024,activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10,activation='softmax')     \n",
        "])"
      ],
      "metadata": {
        "id": "rG9IBYG5XHZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.optimizers.SGD(lr=0.001),\n",
        "    metrics=['accuracy']    \n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "r2vZ-KAJVPQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "    train_ds,\n",
        "    epochs=50,\n",
        "    validation_data=test_ds,\n",
        "    validation_freq=1\n",
        ")"
      ],
      "metadata": {
        "id": "G0Op91mTVeLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.history.history.keys()"
      ],
      "metadata": {
        "id": "1x0xx3GvXO-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax=plt.subplots(2,1,figsize=(10,10)) \n",
        "\n",
        "#Assigning the first subplot to graph training loss and validation loss\n",
        "ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n",
        "ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n",
        "\n",
        "#Plotting the training accuracy and validation accuracy\n",
        "ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
        "ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ReBxHUEdXQPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GoogLetNet\n",
        "\n",
        "GoogLeNet은 2014년 이미지넷 이미지 인식 대회(ILSVRC)에서 VGGNet(VGG19)을 이기고 우승을 차지한 알고리즘이다. GoogLeNet은 19층의 VGG19보다 좀 더 깊은 22층으로 구성되어 있다. GoogLeNet의 original 논문은 Christian Szegedy 등에 의해 2015 CVPR에 개제된 \"Going Deeper with Convolutions\"이다. GoogLeNet이란 이름에서 알 수 있듯이 구글이 이 알고리즘 개발에 참여했다. "
      ],
      "metadata": {
        "id": "lvgn7s66f75G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "OfMVWTFwsOdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "고화질 그림: https://drive.google.com/file/d/1FMJXtnhn8MyvroNn6wysfA3kDZkOALQ6/view?usp=sharing"
      ],
      "metadata": {
        "id": "0o9eDKFowQtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "GoogLeNet은 상술한 바와 같이 22개 층으로 구성되어 있다. 파란색 블럭의 층수를 세보면 22개 층임을 알 수 있다. 이제 GoogLeNet의 특징들을 하나하나 살펴보자. "
      ],
      "metadata": {
        "id": "ab8qqaXXgcX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) 1 x 1 컨볼루션***\n",
        "\n",
        "먼저 주목해야할 것은 1 x 1 사이즈의 필터로 컨볼루션해주는 것이다. 구조도를 보면 곳곳에 1 x 1 컨볼루션 연산이 있음을 확인할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "XbLXJzNWsM-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet_2.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "mVPp3cmtwVuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 x 1 컨볼루션은 어떤 의미를 갖는 것일까? 왜 해주는 것일까? GoogLeNet에서 1 x 1 컨볼루션은 특성맵의 갯수를 줄이는 목적으로 사용된다. 특성맵의 갯수가 줄어들면 그만큼 연산량이 줄어든다.\n",
        "\n",
        " \n",
        "\n",
        "예를 들어, 480장의 14 x 14 사이즈의 특성맵(14 x 14 x 480)이 있다고 가정해보자. 이것을 48개의 5 x 5 x 480의 필터커널로 컨볼루션을 해주면 48장의 14 x 14의 특성맵(14 x 14 x 48)이 생성된다. (zero padding을 2로, 컨볼루션 보폭은 1로 설정했다고 가정했다.) 이때 필요한 연산횟수는 얼마나 될까? 바로 (14 x 14 x 48) x (5 x 5 x 480) = 약 112.9M이 된다. \n",
        "\n",
        " \n",
        "\n",
        "이번에는 480장의 14 x 14 특성맵(14 x 14 x 480)을 먼저 16개의 1 x 1 x 480의 필터커널로 컨볼루션을 해줘 특성맵의 갯수를 줄여보자. 결과적으로 16장의 14 x 14의 특성맵(14 x 14 x 16)이 생성된다. 480장의 특성맵이 16장의 특성맵으로 줄어든 것에 주목하자. 이 14 x 14 x 16 특성맵을 48개의 5 x 5 x 16의 필터커널로 컨볼루션을 해주면 48장의 14 x 14의 특성맵(14 x 14 x 48)이 생성된다. 위에서 1 x 1 컨볼루션이 없을 때와 결과적으로 산출된 특성맵의 크기와 깊이는 같다는 것을 확인하자. 그럼 이때 필요한 연산횟수는 얼마일까? (14 x 14 x 16)*(1 x 1 x 480) + (14 x 14 x 48)*(5 x 5 x 16) = 약 5.3M이다. 112.9M에 비해 훨씬 더 적은 연산량을 가짐을 확인할 수 있다. 연산량을 줄일 수 있다는 점은 네트워크를 더 깊이 만들수 있게 도와준다는 점에서 중요하다. \n",
        "\n"
      ],
      "metadata": {
        "id": "6512UvwTsQPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet_3.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "PIKqlQOpsR2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2) Inception 모듈***\n",
        "\n",
        "\n",
        "이번엔 GoogLeNet의 핵심인 Inception 모듈에 대해 살펴보자. Inception모듈들을 위 구조도에서 표시하면 다음과 같다. \n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet_4.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "-jQj84v6sTFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GoogLeNet은 총 9개의 인셉션 모듈을 포함하고 있다. 인셉션 모듈을 하나 확대해서 자세히 살펴보자. "
      ],
      "metadata": {
        "id": "S3xjxlC3sVam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet_5.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "lRz1WxgWxkuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GoogLeNet에 실제로 사용된 모듈은 1x1 컨볼루션이 포함된 (b) 모델이다. 아까 살펴봤듯이 1x1 컨볼루션은 특성맵의 장수를 줄여주는 역할을 한다. 노란색 블럭으로 표현된 1x1 컨볼루션을 제외한 나이브(naive) 버전을 살펴보면, 이전 층에서 생성된 특성맵을 1x1 컨볼루션, 3x3 컨볼루션, 5x5 컨볼루션, 3x3 최대풀링해준 결과 얻은 특성맵들을 모두 함께 쌓아준다. AlexNet, VGGNet 등의 이전 CNN 모델들은 한 층에서 동일한 사이즈의 필터커널을 이용해서 컨볼루션을 해줬던 것과 차이가 있다. 따라서 좀 더 다양한 종류의 특성이 도출된다. 여기에 1x1 컨볼루션이 포함되었으니 당연히 연산량은 많이 줄어들었을 것이다. \n",
        "\n"
      ],
      "metadata": {
        "id": "BOVtXzB_sXUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) global average pooling\n",
        "\n",
        "\n",
        "AlexNet, VGGNet 등에서는 fully connected (FC) 층들이 망의 후반부에 연결되어 있다. 그러나 GoogLeNet은 FC 방식 대신에 global average pooling이란 방식을 사용한다. global average pooling은 전 층에서 산출된 특성맵들을 각각 평균낸 것을 이어서 1차원 벡터를 만들어주는 것이다. 1차원 벡터를 만들어줘야 최종적으로 이미지 분류를 위한 softmax 층을 연결해줄 수 있기 때문이다. 만약 전 층에서 1024장의 7 x 7의 특성맵이 생성되었다면, 1024장의 7 x 7 특성맵 각각 평균내주어 얻은 1024개의 값을 하나의 벡터로 연결해주는 것이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "_7B60rFesZGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet_6.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "hubPwPEdxuCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 해줌으로 얻을 수 있는 장점은 가중치의 갯수를 상당히 많이 없애준다는 것이다. 만약 FC 방식을 사용한다면 훈련이 필요한 가중치의 갯수가 7 x 7 x 1024 x 1024 = 51.3M이지만 global average pooling을 사용하면 가중치가 단 한개도 필요하지 않다. \n",
        "\n"
      ],
      "metadata": {
        "id": "qq5gO39ssa_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4) auxiliary classifier***\n",
        "\n",
        "\n",
        "네트워크의 깊이가 깊어지면 깊어질수록 vanishing gradient 문제를 피하기 어려워진다. 그러니까 가중치를 훈련하는 과정에 역전파(back propagation)를 주로 활용하는데, 역전파과정에서 가중치를 업데이트하는데 사용되는 gradient가 점점 작아져서 0이 되어버리는 것이다. 따라서 네트워크 내의 가중치들이 제대로 훈련되지 않는다. 이 문제를 극복하기 위해서 GoogLeNet에서는 네트워크 중간에 두 개의 보조 분류기(auxiliary classifier)를 달아주었다. \n",
        "\n"
      ],
      "metadata": {
        "id": "Jkh8Kxyvsch5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/GoogLetNet_7.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "UBPL_vQgxz3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "보조 분류기의 구성을 살펴보면, 5 x 5 평균 풀링(stride 3) -> 128개 1x1 필터커널로 컨볼루션 -> 1024 FC 층 -> 1000 FC 층 -> softmax 순이다. 이 보조 분류기들은 훈련시에만 활용되고 사용할 때는 제거해준다. \n",
        "\n"
      ],
      "metadata": {
        "id": "BzTAfMzHsdwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import AveragePooling2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def conv_module(input,No_of_filters,filtersizeX,filtersizeY,stride,chanDim,padding=\"same\"):\n",
        "  input = Conv2D(No_of_filters,(filtersizeX,filtersizeY),strides=stride,padding=padding)(input)\n",
        "  input = tf.keras.layers.BatchNormalization(axis=chanDim)(input)\n",
        "  input = Activation(\"relu\")(input)\n",
        "  return input\n"
      ],
      "metadata": {
        "id": "uqCzMAmAj5VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_module(input,numK1x1,numK3x3,numk5x5,numPoolProj,chanDim):\n",
        "                                 #Step 1\n",
        "  conv_1x1 = conv_module(input, numK1x1, 1, 1,(1, 1), chanDim) \n",
        "                                 #Step 2\n",
        "  conv_3x3 = conv_module(input, numK3x3, 3, 3,(1, 1), chanDim)\n",
        "  conv_5x5 = conv_module(input, numk5x5, 5, 5,(1, 1), chanDim)\n",
        "                                 #Step 3\n",
        "  pool_proj = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input)\n",
        "  pool_proj = Conv2D(numPoolProj, (1, 1), padding='same', activation='relu')(pool_proj)\n",
        "                                 #Step 4\n",
        "  input = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=chanDim)\n",
        "  return input\n"
      ],
      "metadata": {
        "id": "TvqGhazVj8D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample_module(input,No_of_filters,chanDim):\n",
        "  conv_3x3=conv_module(input,No_of_filters,3,3,(2,2),chanDim,padding=\"valid\")\n",
        "  pool = MaxPooling2D((3,3),strides=(2,2))(input)\n",
        "  input = concatenate([conv_3x3,pool],axis=chanDim)\n",
        "  return input\n"
      ],
      "metadata": {
        "id": "Q0VaZjP2j9g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MiniGoogleNet(width,height,depth,classes):\n",
        "  inputShape=(height,width,depth)\n",
        "  chanDim=-1\n",
        "\n",
        "  # (Step 1) Define the model input\n",
        "  inputs = Input(shape=inputShape)\n",
        "\n",
        "  # First CONV module\n",
        "  x = conv_module(inputs, 96, 3, 3, (1, 1),chanDim)\n",
        "\n",
        "  # (Step 2) Two Inception modules followed by a downsample module\n",
        "  x = inception_module(x, 32, 32,32,32,chanDim)\n",
        "  x = inception_module(x, 32, 48, 48,32,chanDim)\n",
        "  x = downsample_module(x, 80, chanDim)\n",
        "  \n",
        "  # (Step 3) Five Inception modules followed by a downsample module\n",
        "  x = inception_module(x, 112, 48, 32, 48,chanDim)\n",
        "  x = inception_module(x, 96, 64, 32,32,chanDim)\n",
        "  x = inception_module(x, 80, 80, 32,32,chanDim)\n",
        "  x = inception_module(x, 48, 96, 32,32,chanDim)\n",
        "  x = inception_module(x, 112, 48, 32, 48,chanDim)\n",
        "  x = downsample_module(x, 96, chanDim)\n",
        "\n",
        "  # (Step 4) Two Inception modules followed\n",
        "  x = inception_module(x, 176, 160,96,96, chanDim)\n",
        "  x = inception_module(x, 176, 160, 96,96,chanDim)\n",
        "  \n",
        "  # Global POOL and dropout\n",
        "  x = AveragePooling2D((7, 7))(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  # (Step 5) Softmax classifier\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(classes)(x)\n",
        "  x = Activation(\"softmax\")(x)\n",
        "\n",
        "  # Create the model\n",
        "  model = Model(inputs, x, name=\"googlenet\")\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "P9-juxjLkACC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GjAtU5zWmpzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 50\n",
        "INIT_LR = 5e-3\n",
        "def poly_decay(epoch):\n",
        "  maxEpochs = NUM_EPOCHS\n",
        "  baseLR = INIT_LR\n",
        "  power = 1.0\n",
        "  alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\n",
        "  return alpha\n"
      ],
      "metadata": {
        "id": "I1WoysAbkCvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\")\n",
        "testX = testX.astype(\"float\")\n",
        "                                # Step 1\n",
        "mean = np.mean(trainX, axis=0)\n",
        "trainX -= mean\n",
        "testX -= mean\n",
        "                                # Step 2\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "                                # Step 3\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1, horizontal_flip=True,fill_mode=\"nearest\")\n"
      ],
      "metadata": {
        "id": "E4sRZPQVkEDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks=[LearningRateScheduler(poly_decay)]\n",
        "opt = SGD(lr=INIT_LR, momentum=0.9)\n",
        "model = MiniGoogleNet(width=32, height=32, depth=3, classes=10)\n",
        "                                    # Step 1\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "                                    # Step 2\n",
        "model.fit(aug.flow(trainX, trainY, batch_size=64),validation_data=(testX, testY), steps_per_epoch=len(trainX) // 64,epochs=NUM_EPOCHS, callbacks=callbacks, verbose=1)\n"
      ],
      "metadata": {
        "id": "cgPJfuWukFar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=model.evaluate(testX,testY)\n",
        "print('Test Score=',score[0])\n",
        "print('Test Accuracy=',score[1])"
      ],
      "metadata": {
        "id": "jzflLzTFkHCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGGNet의 구조"
      ],
      "metadata": {
        "id": "HQqplEtsyDew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2014년 이미지넷 이미지 인식 대회에서 준우승을 한 모델이다. 여기서 말하는 VGGNet은 16개 또는 19개의 층으로 구성된 모델을 의미한다(VGG16, VGG19로 불림.  이 VGGNet의 original 논문은 Karen Simonyan과 Andrew Zisserman에 의해 2015 ICLR에 게재된 \"Very deep convolutional networks for large-scale image recognition\"이다. "
      ],
      "metadata": {
        "id": "QhDr2_hPyRl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGGNet의 original 논문의 개요에서 밝히고 있듯이 이 연구의 핵심은 네트워크의 깊이를 깊게 만드는 것이 성능에 어떤 영향을 미치는지를 확인하고자 한 것이다. VGG 연구팀은 깊이의 영향만을 최대한 확인하고자 컨볼루션 필터커널의 사이즈는 가장 작은 3 x 3으로 고정했다.\n",
        "\n",
        " \n",
        "\n",
        "개인적으로 나는 모든 필터 커널의 사이즈를 3 x 3으로 설정했기 때문에 네트워크의 깊이를 깊게 만들 수 있다고 생각한다. 왜냐하면 필터커널의 사이즈가 크면 그만큼 이미지의 사이즈가 금방 축소되기 때문에 네트워크의 깊이를 충분히 깊게 만들기 불가능하기 때문이다. \n",
        "\n",
        " \n",
        "\n",
        "VGG 연구팀은 original 논문에서 총 6개의 구조(A, A-LRN, B, C, D, E)를 만들어 성능을 비교했다. 여러 구조를 만든 이유는 기본적으로 깊이의 따른 성능 변화를 비교하기 위함이다. 이중 D 구조가 VGG16이고 E 구조가 VGG19라고 보면 된다. \n",
        "\n"
      ],
      "metadata": {
        "id": "SVjtwYNpyVXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/VGG_1.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "p7AfoZ5eya41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG 연구팀은 AlexNet과 VGG-F, VGG-M, VGG-S에서 사용되던 Local Response Normalization(LRN)이 A 구조와 A-LRN 구조의 성능을 비교함으로 성능 향상에 별로 효과가 없다고 실험을 통해 확인했다. 그래서 더 깊은 B, C, D, E 구조에는 LRN을 적용하지 않는다고 논문에서 밝혔다. 또한 그들은 깊이가 11층, 13층, 16층, 19층으로 깊어지면서 분류 에러가 감소하는 것을 관찰했다. 즉, 깊어질수록 성능이 좋아진다는 것이었다. \n",
        "\n",
        " \n",
        "\n",
        "VGGNet의 구조를 깊이 들여다보기에 앞서 먼저 집고 넘어가야할 것이 있다. 그것은 바로 3 x 3 필터로 두 차례 컨볼루션을 하는 것과 5 x 5 필터로 한 번 컨볼루션을 하는 것이 결과적으로 동일한 사이즈의 특성맵을 산출한다는 것이다(아래 그림 참고). 3 x 3 필터로 세 차례 컨볼루션 하는 것은 7 x 7 필터로 한 번 컨볼루션 하는 것과 대응된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "3oq4EcRyyaZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/VGG_2.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "2AID1pmeyb9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러면 3 x 3 필터로 세 차례 컨볼루션을 하는 것이 7 x 7 필터로 한 번 컨볼루션하는 것보다 나은 점은 무엇일까? 일단 가중치 또는 파라미터의 갯수의 차이다. 3 x 3 필터가 3개면 총 27개의 가중치를 갖는다. 반면 7 x 7 필터는 49개의 가중치를 갖는다. CNN에서 가중치는 모두 훈련이 필요한 것들이므로, 가중치가 적다는 것은 그만큼 훈련시켜야할 것의 갯수가 작아진다. 따라서 학습의 속도가 빨라진다. 동시에 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가시키기 때문에 특성이 점점 더 유용해진다. "
      ],
      "metadata": {
        "id": "VP-CuRzAyfSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러면 이제 VGG16(D 구조)를 예시로, 각 층마다 어떻게 특성맵이 생성되고 변화되는지 자세하게 살펴보자. 아래 구조도와 함께 각 층의 세부사항을 읽어나가면 이해하기가 그렇게 어렵지 않을 것이다. "
      ],
      "metadata": {
        "id": "iXPptPZAyhW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/VGG_3.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "a5WdxGlDyiPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) 인풋: 224 x 224 x 3 이미지(224 x 224 RGB 이미지)를 입력받을 수 있다.\n",
        "\n",
        " \n",
        "\n",
        "1) 1층(conv1_1): 64개의 3 x 3 x 3 필터커널로 입력이미지를 컨볼루션해준다. zero padding은 1만큼 해줬고, 컨볼루션 보폭(stride)는 1로 설정해준다. zero padding과 컨볼루션 stride에 대한 설정은 모든 컨볼루션층에서 모두 동일하니 다음 층부터는 설명을 생략하겠다. 결과적으로 64장의 224 x 224 특성맵(224 x 224 x 64)들이 생성된다. 활성화시키기 위해 ReLU 함수가 적용된다. ReLU함수는 마지막 16층을 제외하고는 항상 적용되니 이 또한 다음 층부터는 설명을 생략하겠다. \n",
        "\n",
        " \n",
        "\n",
        "2) 2층(conv1_2): 64개의 3 x 3 x 64 필터커널로 특성맵을 컨볼루션해준다. 결과적으로 64장의 224 x 224 특성맵들(224 x 224 x 64)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용함으로 특성맵의 사이즈를 112 x 112 x 64로 줄인다. \n",
        "\n",
        " \n",
        "\n",
        "*conv1_1, conv1_2와 conv2_1, conv2_2등으로 표현한 이유는 해상도를 줄여주는 최대 풀링 전까지의 층등을 한 모듈로 볼 수 있기 때문이다.  \n",
        "\n",
        " \n",
        "\n",
        "3) 3층(conv2_1): 128개의 3 x 3 x 64 필터커널로 특성맵을 컨볼루션해준다. 결과적으로 128장의 112 x 112 특성맵들(112 x 112 x 128)이 산출된다. \n",
        "\n",
        " \n",
        "\n",
        "4) 4층(conv2_2): 128개의 3 x 3 x 128 필터커널로 특성맵을 컨볼루션해준다. 결과적으로 128장의 112 x 112 특성맵들(112 x 112 x 128)이 산출된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용해준다. 특성맵의 사이즈가 56 x 56 x 128로 줄어들었다.\n",
        "\n",
        " \n",
        "\n",
        "5) 5층(conv3_1): 256개의 3 x 3 x 128 필터커널로 특성맵을 컨볼루션한다. 결과적으로 256장의 56 x 56 특성맵들(56 x 56 x 256)이 생성된다. \n",
        "\n",
        " \n",
        "\n",
        "6) 6층(conv3_2): 256개의 3 x 3 x 256 필터커널로 특성맵을 컨볼루션한다. 결과적으로 256장의 56 x 56 특성맵들(56 x 56 x 256)이 생성된다. \n",
        "\n",
        " \n",
        "\n",
        "7) 7층(conv3_3): 256개의 3 x 3 x 256 필터커널로 특성맵을 컨볼루션한다. 결과적으로 256장의 56 x 56 특성맵들(56 x 56 x 256)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용한다. 특성맵의 사이즈가 28 x 28 x 256으로 줄어들었다. \n",
        "\n",
        " \n",
        "\n",
        "8) 8층(conv4_1): 512개의 3 x 3 x 256 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 28 x 28 특성맵들(28 x 28 x 512)이 생성된다. \n",
        "\n",
        " \n",
        "\n",
        "9) 9층(conv4_2): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 28 x 28 특성맵들(28 x 28 x 512)이 생성된다. \n",
        "\n",
        " \n",
        "\n",
        "10) 10층(conv4_3): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 28 x 28 특성맵들(28 x 28 x 512)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용한다. 특성맵의 사이즈가 14 x 14 x 512로 줄어든다.\n",
        "\n",
        " \n",
        "\n",
        "11) 11층(conv5_1): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 14 x 14 특성맵들(14 x 14 x 512)이 생성된다.\n",
        "\n",
        " \n",
        "\n",
        "12) 12층(conv5_2): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 14 x 14 특성맵들(14 x 14 x 512)이 생성된다.\n",
        "\n",
        " \n",
        "\n",
        "13) 13층(conv5-3): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 14 x 14 특성맵들(14 x 14 x 512)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용한다. 특성맵의 사이즈가 7 x 7 x 512로 줄어든다.\n",
        "\n",
        " \n",
        "\n",
        "14) 14층(fc1): 7 x 7 x 512의 특성맵을 flatten 해준다. flatten이라는 것은 전 층의 출력을 받아서 단순히 1차원의 벡터로 펼쳐주는 것을 의미한다. 결과적으로 7 x 7 x 512 = 25088개의 뉴런이 되고, fc1층의 4096개의 뉴런과 fully connected 된다. 훈련시 dropout이 적용된다.\n",
        "\n",
        "(이 부분을 제대로 이해할 수 있도록 지적해주신 jaist님과 hiska님께 감사드립니다.^^)\n",
        "\n",
        " \n",
        "\n",
        "15) 15층(fc2): 4096개의 뉴런으로 구성해준다. fc1층의 4096개의 뉴런과 fully connected 된다. 훈련시 dropout이 적용된다. \n",
        "\n",
        " \n",
        "\n",
        "16) 16층(fc3): 1000개의 뉴런으로 구성된다. fc2층의 4096개의 뉴런과 fully connected된다. 출력값들은 softmax 함수로 활성화된다. 1000개의 뉴런으로 구성되었다는 것은 1000개의 클래스로 분류하는 목적으로 만들어진 네트워크란 뜻이다. \n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "bA7nVlXDyl2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        " \n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "NUM_epochs = 10\n",
        " \n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# One hot Encoding\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        " \n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QgwMuF2jnGxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        " \n",
        "hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=NUM_epochs, batch_size=batch_size, verbose=2)\n",
        " \n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
        " \n",
        "#모델 시각\n",
        "fig, loss_ax = plt.subplots()\n",
        " \n",
        "acc_ax = loss_ax.twinx()\n",
        " \n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        " \n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        " \n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax. set_ylabel('accuracy')\n",
        " \n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iju9wdHZnxDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "K-_TA7N6oLab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2015년 ILSVRC에서 우승을 차지한 ResNet에 대해서 소개하려고 한다.층수에 있어서 ResNet은 급속도로 깊어진다. 2014년의 GoogLeNet이 22개 층으로 구성된 것에 비해, ResNet은 152개 층을 갖는다. 약 7배나 깊어졌다! "
      ],
      "metadata": {
        "id": "GNZTCh7goQo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/ResNet.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "qfYAT8vXowfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그림을 보면 네트워크가 깊어지면서 top-5 error가 낮아진 것을 확인할 수 있다. 한마디로 성능이 좋아진 것이다. 그렇다면 질문! \"망을 깊게하면 무조건 성능이 좋아질까?\" 이것을 확인하기 위해 ResNet의 저자들은 컨볼루션 층들과 fully-connected 층들로 20 층의 네트워크와 56층의 네트워크를 각각 만든 다음에 성능을 테스트해보았다. "
      ],
      "metadata": {
        "id": "J4so3Jweo210"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/ResNet_1.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "nB-0Q0uJo9CR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그래프들을 보면 오히려 더 깊은 구조를 갖는 56층의 네트워크가 20층의 네트워크보다 더 나쁜 성능을 보임을 알 수 있다. 기존의 방식으로는 망을 무조건 깊게 한다고 능사가 아니라는 것을 확인한 것이다. 뭔가 새로운 방법이 있어야 망을 깊게 만드는 효과를 볼 수 있다는 것을 ResNet의 저자들은 깨달았다. "
      ],
      "metadata": {
        "id": "Hu1l4t0Ko-Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) Residual Block***\n"
      ],
      "metadata": {
        "id": "J2dIu2NTpCUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그것이 바로 ResNet의 핵심인 Residual Block의 출현을 가능케 했다. 아래 그림에서 오른쪽이 Residual Block을 나타낸다. 기존의 망과 차이가 있다면 입력값을 출력값에 더해줄 수 있도록 지름길(shortcut)을 하나 만들어준 것 뿐이다. "
      ],
      "metadata": {
        "id": "q3kQQficpW2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/ResNet_2.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "_DS0MHygpZvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존의 신경망은 입력값 x를 타겟값 y로 매핑하는 함수 H(x)를 얻는 것이 목적이었다. 그러나 ResNet은 F(x) + x를 최소화하는 것을 목적으로 한다. x는 현시점에서 변할 수 없는 값이므로 F(x)를 0에 가깝게 만드는 것이 목적이 된다. F(x)가 0이 되면 출력과 입력이 모두 x로 같아지게 된다. F(x) = H(x) - x이므로 F(x)를 최소로 해준다는 것은 H(x) - x를 최소로 해주는 것과 동일한 의미를 지닌다. 여기서 H(x) - x를 잔차(residual)라고 한다. 즉, 잔차를 최소로 해주는 것이므로 ResNet이란 이름이 붙게 된다. \n",
        "\n"
      ],
      "metadata": {
        "id": "4uRkbu2RpZOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet은 기본적으로 VGG-19의 구조를 뼈대로 한다. 거기에 컨볼루션 층들을 추가해서 깊게 만든 후에, shortcut들을 추가하는 것이 사실상 전부다. 34층의 ResNet과 거기에서 shortcut들을 제외한 버전인 plain 네트워크의 구조는 다음과 같다. "
      ],
      "metadata": {
        "id": "-W8htU4Zp0Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/ResNet_3.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "H6qjSdM4qIfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그림을 보면 알 수 있듯이 34층의 ResNet은 처음을 제외하고는 균일하게 3 x 3 사이즈의 컨볼루션 필터를 사용했다. 그리고 특성맵의 사이즈가 반으로 줄어들 때, 특성맵의 뎁스를 2배로 높였다. \n",
        "\n",
        " \n",
        "\n",
        "저자들은 과연 shortcut, 즉 Residual block들이 효과가 있는지를 알기 위해 이미지넷에서 18층 및 34층의 plain 네트워크와 ResNet의 성능을 비교했다. \n",
        "\n"
      ],
      "metadata": {
        "id": "2bnQRcsOqO4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/ResNet_4.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "mEkzv_CYqU9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "왼쪽 그래프를 보면 plain 네트워크는 망이 깊어지면서 오히려 에러가 커졌음을 알 수 있다. 34층의 plain 네트워크가 18층의 plain 네트워크보다 성능이 나쁘다. 반면, 오른쪽 그래프의 ResNet은 망이 깊어지면서 에러도 역시 작아졌다! shortcut을 연결해서 잔차(residual)를 최소가 되게 학습한 효과가 있다는 것이다. \n",
        "\n",
        " \n",
        "\n",
        "아래 표는 18층, 34층, 50층, 101층, 152층의 ResNet이 어떻게 구성되어 있는가를 잘 나타내준다. 깊은 구조일수록 성능도 좋다. 즉, 152층의 ResNet이 가장 성능이 뛰어나다. \n",
        "\n"
      ],
      "metadata": {
        "id": "YshaRdWQqT9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__) # 2.4.1\n",
        "\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "metadata": {
        "id": "F8yv9UlMwTEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "IMAGE_WIDTH = 32\n",
        "IMAGE_HEIGHT = 32\n",
        "NUM_CLASSES = 10\n",
        "CHANNELS = 3\n"
      ],
      "metadata": {
        "id": "zp8TOxpdwTjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "def generate_datasets():\n",
        "  (X_train, y_train), (X_test, y_test) = cifar10.load_data() # cifar 10 데이터셋 로드\n",
        "  X_test, y_test = resize_and_rescale(X_test, y_test) \n",
        "  X_train, y_train = augment(X_train, y_train) # data augmentation\n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "def resize_and_rescale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = tf.image.resize(image, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
        "\t# training image의 intensity mean 값이 있다면 load, 없으면 구한다.\n",
        "  if os.path.exists('pixel_mean_value.npy'): \n",
        "    mean_image = np.load('pixel_mean_value.npy')\n",
        "  else:\n",
        "    mean_image = np.mean(image, axis=0)\n",
        "  image -= mean_image # per-pixel mean subtraction\n",
        "  image = (image / 128.0) # normalization to [-1,1]\n",
        "  return image, label\n",
        "\n",
        "def augment(image,label):\n",
        "  image, label = resize_and_rescale(image, label)\n",
        "  # Add 4 pixels of padding\n",
        "  image = tf.map_fn(lambda img: tf.image.resize_with_crop_or_pad(img, IMAGE_WIDTH + 4, IMAGE_HEIGHT + 4), image)\n",
        "   # Random crop back to the original size\n",
        "  image = tf.map_fn(lambda img: tf.image.random_crop(img, size=[IMAGE_WIDTH, IMAGE_HEIGHT, 3]), image)\n",
        "  image = tf.map_fn(lambda img: tf.image.random_flip_left_right(img), image)  \n",
        "  return image, label\n"
      ],
      "metadata": {
        "id": "PjKpeT-GwU5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(tf.keras.layers.Layer):  \n",
        "  def __init__(self, filter_num, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "    l2 = tf.keras.regularizers.l2(0.0001)\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=filter_num, kernel_size=(3,3), \n",
        "                  strides=stride, padding='same', kernel_regularizer=l2, \n",
        "                  kernel_initializer=initializer)\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters=filter_num, kernel_size=(3,3),\n",
        "                  strides=1, padding='same', kernel_regularizer=l2, \n",
        "                  kernel_initializer=initializer)\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "    if stride != 1:\n",
        "      self.downsample = tf.keras.Sequential()\n",
        "      self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num, kernel_size=(1,1),\n",
        "                  strides=stride, kernel_regularizer=l2,\n",
        "                  kernel_initializer=initializer))\n",
        "      self.downsample.add(tf.keras.layers.BatchNormalization())\n",
        "    else:\n",
        "      self.downsample = lambda x: x\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    residual = self.downsample(inputs)\n",
        "\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.bn1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "\n",
        "    output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
        "\n",
        "    return output\n",
        "\n",
        "def make_basic_block_layer(filter_num, blocks, stride=1):\n",
        "  res_block = tf.keras.Sequential()\n",
        "  res_block.add(BasicBlock(filter_num, stride=stride))\n",
        "\n",
        "  for _ in range(1, blocks):\n",
        "    res_block.add(BasicBlock(filter_num, stride=1))\n",
        "\n",
        "  return res_block"
      ],
      "metadata": {
        "id": "MT_WyT-nwXBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(tf.keras.Model):  \n",
        "  def __init__(self, layer_params):\n",
        "    super(ResNet, self).__init__()\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "    l2 = tf.keras.regularizers.l2(0.0001)\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=1, \n",
        "                    padding='same', kernel_regularizer=l2, kernel_initializer=initializer)\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "    self.layer1 = make_basic_block_layer(filter_num=16, blocks=layer_params[0], stride=2)\n",
        "    self.layer2 = make_basic_block_layer(filter_num=32, blocks=layer_params[1], stride=2)\n",
        "    self.layer3 = make_basic_block_layer(filter_num=64, blocks=layer_params[2], stride=2)\n",
        "    self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=tf.keras.activations.softmax,\n",
        "                    kernel_regularizer=l2, kernel_initializer=initializer)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.bn1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.avgpool(x)\n",
        "    output = self.fc(x)\n",
        "\n",
        "    return output\n",
        "\n",
        "def get_model():\n",
        "  model = ResNet(layer_params=[3, 3, 3]) # basic block으로 3쌍의 conv-bn layer를 쌓음\n",
        "  model.build(input_shape=(None, IMAGE_WIDTH, IMAGE_HEIGHT, CHANNELS))\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=False), \n",
        "                    loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "L_eDwuDrwZal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = generate_datasets()\n",
        "\n",
        "print('train input shape: ', X_train.shape) # (50000, 32, 32, 3)\n",
        "print('test input shape: ', X_test.shape) # (10000, 32, 32, 3)\n",
        "print('train output shape: ', y_train.shape) # (50000, 1)\n",
        "print('train oupput shape: ', y_train.shape) # (50000, 1)\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.001)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_split=0.1, \n",
        "\t\t\t\t\tbatch_size=BATCH_SIZE, callbacks=[lr_schedule, early_stopping])\n"
      ],
      "metadata": {
        "id": "LXllrSIGwd5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.ylim(0, 3)\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], 'b-', label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'r--', label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ELQj3FWuwgQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('ResNet_20.h5') # 모델을 코랩에 저장\n",
        "\n",
        "from google.colab import files\n",
        "files.download('ResNet_20.h5') # 코랩에 저장된 모델을 로컬로 다운로드"
      ],
      "metadata": {
        "id": "7lqqbEoVwhjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENet"
      ],
      "metadata": {
        "id": "LJVdEpb8qtVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENet은 2017년 이미지넷 대회에서 우승을 차지한 모델입니다. top-5 error가 2.251%밖에 되지 않습니다. ground-truth인 사람의 top-5 error가 5%라고 하니 대단한 결과죠. 사람보다 이 데이터셋에서만큼은 더 이미지 분류를 잘 해낸다는 뜻입니다. "
      ],
      "metadata": {
        "id": "ON9pNTFKqx6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENet의 original 논문은 2018년 CVPR에서 발표된 \"Squeeze-and-Excitation Networks\"입니다. 한국어로 번역해보면 짜내고(squeeze) 활성화시키는(excitatation) 망이라고 볼 수 있습니다. 야구에서 보통 스퀴즈라고 하면 점수를 어떻게든 \"짜내기\" 위해 희생번트를 대는 등의 전술을 의미하죠? 이것과 SENet의 Squeeze와는 큰 관련은 없지만, Squeeze의 의미는 알게 되셨을 것이라고 생각합니다"
      ],
      "metadata": {
        "id": "rNaPckhmq2Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock.png\"/>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "-pZPTy5csVHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러면 이제 SENet에 대한 이야기를 시작하도록 하겠습니다. SENet 논문은 기존 어떤 모델에도 적용될 수 있는 SE block이라는 것을 제안했습니다. VGGNet, GoogLeNet, ResNet 등에 SE block을 첨가함으로써 성능 향상을 도모한 것입니다. 성능은 꽤 많이 향상되는 반면, hyperparameter는 많이 늘지 않기 때문에, 연산량 증가는 크지 않습니다. 보통 성능 향상을 시키려면 연산량도 그만큼 엄청나게 증가되는데, SENet의 경우 연산량은 크게 늘지 않으면서도 분류 정확도를 높일 수 있기 때문에 SE block을 사용하는 것은 효율적이라고 볼 수 있습니다. "
      ],
      "metadata": {
        "id": "NTleqRttq4bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러면, 위 그림을 보면서 SE block에 대해서 설명하겠습니다. $X$는 특성맵이고 U도 특성맵입니다. $F_{tr}$ 은 컨볼루션으로 보시면 됩니다. $H' \\times W' \\times C'$ 크기의 특성맵 $X$가 $F_{tr}$ 컨볼루션을 통해  $H \\times W \\times C$ 크기의 특성맵 $U$가 됩니다.\n",
        "\n",
        "그 다음에 스퀴즈(squeeze)를 실행해줍니다. $C$개 채널의 2차원($H \\times W$)의 특성맵들을 1x1 사이즈의 특성맵으로 변환해주는 것입니다. 간단히 global average pooling (GAP)을 통해 각 2차원의 특성맵을 평균내어 하나의 값을 얻습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "zLF33zDIq8_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock_1.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "QopM0TGpr_ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이로인해 각 채널을 하나의 숫자로 묘사할 수 있게 됩니다. 총 C개 채널의 특성맵이 있으므로 1x1xC의 특성맵으로 스퀴즈됩니다. 특성맵을 global하게 표현한 것이죠. 컨볼루션이 local 정보를 다룬다면, 스퀴즈는 global 정보를 다룬다고 볼 수 있습니다. \n",
        "\n",
        " \n",
        "\n",
        "스퀴즈 후에는 활성화(excitation) 작업에 돌입합니다. 두 개의 Fully-connected(FC) 층을 더해줘서 각 채널의 상대적 중요도를 알아내는 겁니다. \n",
        "\n"
      ],
      "metadata": {
        "id": "5KrMcnMKsAf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock_2.png\"/>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "ktXzS99lsBCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$σ$는 시그모이드함수이고, $δ$는 ReLU 함수입니다. 위 공식을 살펴보자면, 먼저 스퀴즈를 통해 얻은 $z_c$을 인풋으로 삼아서 %W_1%가 가중치들과 fully-connected하게 곱해줍니다. 그래서 얻은 값을 $δ$, 즉 ReLU 함수로 활성화해준 후에 $W_2$ 가중치들과 fully-connected하게 곱해줍니다. 그렇게 얻은 값을 \n",
        "$σ$, 즉 시그모이드 함수로 활성화해줘서 0과 1사이의 값을 갖게 합니다. 따라서 각 채널의 상대적 중요도를 0과 1의 값으로 파악할 수 있게 됩니다.\n",
        "\n",
        "여기서 기억해야할 것은 FC 층들이 병목(bottle-neck) 구조가 되게 만든다는 것입니다. 은닉(hidden) 층의 뉴런의 갯수를 입력 층보다 작게 해줍니다. 그리고 출력 층의 뉴런의 갯수는 입력 층과 동일하게 해줍니다. 병목 구조로 해주는 이유는 hyper-parameter의 갯수를 많이 늘리지 않기 위함도 있고, 일반화에 도움을 주기 위함도 있습니다. 은닉 층의 뉴런 갯수는 reduction ratio, r값에 의해 결정됩니다. r이 클수록 은닉 층의 뉴런 갯수가 더 적어지고, 복잡도도 더 낮아집니다. 따라서 각 층의 뉴런의 갯수는 채널의 갯수인 $C$에서 시작해서 $C/r$로 감소했다가 다시 $C$로 증가합니다. "
      ],
      "metadata": {
        "id": "IzsJXlSls04q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 얻은 $C$개의 $s_c$를 $u_c$에 각각 곱해줘서 $U$를 재보정해줍니다. 그것을 $\\tilde{X}$ 라고 합니다.  \n"
      ],
      "metadata": {
        "id": "-58B-qxjtNR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock_3.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "mFT6RemYtZPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정리하자면, 특성맵은 $X$에서 컨볼루션을 통해 $U$로, $U$에서 SE block을 통해 \n",
        "$\\tilde{X}$ 로 변환됩니다. \n",
        "\n"
      ],
      "metadata": {
        "id": "Mneki4K8tYnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SE block의 목적은 한마디로 컨볼루션을 통해 생성된 특성을 채널당 중요도를 고려해서 재보정(recalibration)하는 것입니다**. 이러한 SE block을 컨볼루션 연산 뒤에 붙여줌으로써 성능 향상을 도모한 것이 바로 SENet의 핵심입니다. "
      ],
      "metadata": {
        "id": "fcrLeDGytfP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**기존 CNN 모델과의 결합**:\n",
        "위에서 말씀드린대로 SE block은 기존의 CNN 모델들에 붙여서 사용할 수 있습니다. VGGNet의 경우에는 컨볼루션 및 활성화 뒤에 바로 붙이면 되고, GoogLeNet의 경우에는 Inception 모듈 뒤에 붙이면 되고, ResNet의 경우에는 Residual 모듈 뒤에 붙이면 됩니다. 다음과 같이 말이죠."
      ],
      "metadata": {
        "id": "A_7p-2kItg3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock_4.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "q3nYwvRyusF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**성능 분석:** SENet 논문에 성능에 관한 여러 표들이 있지만 가장 중요한 두 개의 표만 보여드리겠습니다. 너무 많은 표를 보여드리면 오히려 이해하는데 방해가 될 수도 있을 것 같아서입니다.  \n",
        "\n"
      ],
      "metadata": {
        "id": "HCsgysPru3mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock_5.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "eC1iKeQuvdpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 표에서 original은 original 논문에서 얻은 결과를, re-implementation은 저자들이 직접 실험해서 얻은 결과를, SENet은 SE block을 첨가해서 얻은 결과를 각각 나타냅니다. 이 표를 통해 알 수 있는 것은 SE block을 첨가했을 때 항상 top-1 error와 top-5 error를 낮췄다는 것입니다. 그리고 알고리즘의 복잡도는 크게 증가하지 않았다는 것도 GFLOPs 열에서 알 수 있습니다. "
      ],
      "metadata": {
        "id": "O5w4zax4ve4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENet이 출시된 당시 최신 알고리즘들과 이미지넷에서의 성능을 비교한 표는 다음과 같습니다. SENet-154는 SE block을 개정된 ResNeXt와 통합한 것을 의미합니다. 이름으로 유추할 수 있는 것은 ResNet을 좀 더 발전시킨 모델 같습니다. 아무튼, SENet-154가 그당시 최신 모델들보다 우월한 성능을 보임을 아래 표를 통해 알 수 있습니다. "
      ],
      "metadata": {
        "id": "IahO0vspvrnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Hyun-chul/DeepLearning/main/SEBlock_6.png\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "MbfmItfHv4ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이것으로 SENet에 관한 설명은 마치려고 합니다. Squeeze와 Excitation이라는 어찌보면 간단한 과정을 통해 성능을 이렇게 향상시킬 수 있다는 것이 놀랍습니다."
      ],
      "metadata": {
        "id": "DWCLdnEAv_C4"
      }
    }
  ]
}
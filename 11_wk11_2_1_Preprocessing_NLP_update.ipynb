{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**참고문헌: 딥 러닝을 이용한 자연어 처리 입문, 02. 텍스트 전처리(Text preprocessing)**"
      ],
      "metadata": {
        "id": "qbh2TpOZM4Y_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTLjVJS5GSaE"
      },
      "source": [
        "#01. 자연어 처리 전처리 이해하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3rkHZRNGV6k"
      },
      "source": [
        "자연어 처리는 일반적으로 토큰화, 단어 집합 생성, 정수 인코딩, 패딩, 벡터화의 과정을 거칩니다. 이번 챕터에서는 이러한 전반적인 과정에 대해서 이해합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK8dCBNDGX1c"
      },
      "source": [
        "## 1. 토큰화(Tokenization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWtdnMY-GrX2"
      },
      "source": [
        "주어진 텍스트를 단어 또는 문자 단위로 자르는 것을 토큰화라고 합니다. 예를 들어 주어진 문장이 다음과 같다고 해봅시다. 영어의 경우 토큰화를 사용하는 도구로서 대표적으로 spaCy와 NLTK가 있습니다. 물론, 파이썬 기본 함수인 split으로 토큰화를 할 수도 있습니다.\n",
        "\n",
        "우선 영어에 대해서 토큰화 실습을 해봅시다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HygPz2OZGtIE"
      },
      "source": [
        "en_text = \"A Dog Run back corner near spare bedrooms\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUWMl6stGu9t"
      },
      "source": [
        "1. spaCy 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QYxrccdGxUL"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv7jXcDaG0S7"
      },
      "source": [
        "def tokenize(en_text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Qo7X0xG2p2"
      },
      "source": [
        "print(tokenize(en_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Sa3gvgG8mf"
      },
      "source": [
        "2. NLTK 사용하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSlJhwXWG9GP"
      },
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(word_tokenize(en_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omA-Z77RHFK3"
      },
      "source": [
        "3. 띄어쓰기로 토큰화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBQKUK4HFsb"
      },
      "source": [
        "print(en_text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bka46hAuHIgn"
      },
      "source": [
        "사실 영어의 경우에는 띄어쓰기 단위로 토큰화를 해도 단어들 간 구분이 꽤나 명확하기 때문에, 토큰화 작업이 수월합니다. 하지만 한국어의 경우에는 토큰화 작업이 훨씬 까다롭습니다. 그 이유는 한국어는 조사, 접사 등으로 인해 단순 띄어쓰기 단위로 나누면 같은 단어가 다른 단어로 인식되어서 단어 집합(vocabulary)의 크기가 불필요하게 커지기 때문입니다.\n",
        "\n",
        "단어 집합(vocabuary)이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미합니다.\n",
        "예를 들어 단어 '사과'가 많이 들어간 어떤 문장에 띄어쓰기 토큰화를 한다면 '사과가', '사과를', '사과의', '사과와', '사과는'과 같은 식으로 같은 단어임에도 조사가 붙어서 다른 단어로 인식될 수 있습니다. 예를 통해 구체적으로 이해해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFTcuwuHLRH"
      },
      "source": [
        "4. 한국어 띄어쓰기 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6sga8DHKnv"
      },
      "source": [
        "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYiOgyijHPUW"
      },
      "source": [
        "print(kor_text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq5b6EmbHTUW"
      },
      "source": [
        "위의 예제에서는 '사과'란 단어가 총 4번 등장했는데 모두 '의', '를', '가', '랑' 등이 붙어있어 이를 제거해주지 않으면 기계는 전부 다른 단어로 인식하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM8WHGXNHWHn"
      },
      "source": [
        "5. 형태소 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fa7XtPuHXNI"
      },
      "source": [
        "위와 같은 상황을 방지하기 위해서 한국어는 보편적으로 '형태소 분석기'로 토큰화를 합니다. 여기서는 형태소 분석기 중에서 mecab을 사용해보겠습니다. 아래의 커맨드로 colab에서 mecab을 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-JdGaf2Hacw"
      },
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "1bOvI0FC4N8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " cd /content/Mecab-ko-for-Google-Colab/Mecab-ko-for-Google-Colab"
      ],
      "metadata": {
        "id": "uQ9QWwF_4QVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "Mwq9bvrg4akg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !bash install_mecab-ko_on_colab_light_220429.sh"
      ],
      "metadata": {
        "id": "yoEoIYq14UpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHsYurzVHcXi"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "# from konlpy.tag import Okt, Mecab\n",
        "\n",
        "tokenizer = Mecab()\n",
        "print(tokenizer.morphs(kor_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10AHTO2sHehA"
      },
      "source": [
        "앞선 예와 다르게 '의', '를', '가', '랑' 등이 전부 분리되어 기계는 '사과'라는 단어를 하나의 단어로 처리할 수 있습니다.\n",
        "\n",
        "지금까지는 단어 또는 형태소 단위로 토큰화를 했지만 이보다도 더 작은 단위인 문자 단위로 토큰화를 수행하는 경우도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOERLUU7HbFY"
      },
      "source": [
        "6. 문자 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jBlYJmaHkGJ"
      },
      "source": [
        "print(list(en_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m--rMHXwHydk"
      },
      "source": [
        "간단히 토큰화에 대해서 알아봤습니다. 이제부터는 좀 더 많은 데이터를 가지고 실습해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuewO0lBH0fs"
      },
      "source": [
        "## 2. 단어 집합(Vocabulary) 생성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfZ_LL5eH4B6"
      },
      "source": [
        "단어 집합(vocabuary)이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미합니다. 우선, 실습을 위해서 깃허브에서 '네이버 영화 리뷰 분류하기' 데이터를 다운로드하겠습니다. 네이버 영화 리뷰 데이터는 총 20만 개의 영화 리뷰를 긍정 1, 부정 0으로 레이블링한 데이터입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPUefYkJH2IS"
      },
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "from konlpy.tag import Mecab\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wxdsavmH8rT"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n",
        "print(data.shape)\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-vlzx3IH_68"
      },
      "source": [
        "print('전체 샘플의 수 : {}'.format(len(data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovVfhVTjIJz-"
      },
      "source": [
        "sample_data = data[:100] # 임의로 100개만 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02DcWlA3IPm9"
      },
      "source": [
        "정규 표현식을 통해서 데이터를 정제합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etuPknj1NpuK"
      },
      "source": [
        "[ㄱ-ㅎ] - ㄱ부터 ㅎ까지 자음 전체\n",
        "\n",
        "[ㅏ-ㅣ] - ㅏ부터 ㅣ까지 모음 전체\n",
        "\n",
        "[가-힣] - 사전순으로 올 수 있는 가 부터 제일 마지막 힣까지 글자 전체\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPOI6p3aIPDu"
      },
      "source": [
        "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "# 한글과 공백을 제외하고 모두 제거\n",
        "sample_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4_lBf_ISyn"
      },
      "source": [
        "형태소 분석기는 mecab을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TAVDAOCJal3"
      },
      "source": [
        "# 불용어 정의\n",
        "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzUuBf4lIUpl"
      },
      "source": [
        "tokenizer = Mecab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uknzbcHHIWdI"
      },
      "source": [
        "tokenized=[]\n",
        "for sentence in sample_data['document']:\n",
        "    temp = tokenizer.morphs(sentence) # 토큰화\n",
        "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
        "    tokenized.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g0aP5HsIYOt"
      },
      "source": [
        "print(tokenized[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgTgWqaPIZrn"
      },
      "source": [
        "이제 단어 집합을 만들어봅시다. NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HYEPzVSIbs3"
      },
      "source": [
        "vocab = FreqDist(np.hstack(tokenized))\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO9AWvpQIdXO"
      },
      "source": [
        "단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ACgs3lrIeC-"
      },
      "source": [
        "vocab['어릴']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5bEfRNnIjJ3"
      },
      "source": [
        "'재밌'이란 단어가 총 10번 등장하였습니다. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴합니다. 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있습니다. 등장 빈도수 상위 500개의 단어만 단어 집합으로 저장해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtJXbVOmIlV-"
      },
      "source": [
        "vocab_size = 500\n",
        "# 상위 vocab_size개의 단어만 보존\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZrwcMGzIm_y"
      },
      "source": [
        "단어 집합의 크기가 500으로 줄어든 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkV2PpP_Iqra"
      },
      "source": [
        "# 3. 각 단어에 고유한 정수 부여\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRbWdHpyItCJ"
      },
      "source": [
        "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴한다는 특징이 있습니다. 인덱스 0과 1은 다른 용도로 남겨두고 나머지 단어들은 2부터 501까지 순차적으로 인덱스를 부여해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zff8t_ZTIo_o"
      },
      "source": [
        "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)\n",
        "\n",
        "word_to_index['pad'] = 1\n",
        "word_to_index['unk'] = 0\n",
        "\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xapkhHFLI2MY"
      },
      "source": [
        "이제 기존의 훈련 데이터에서 각 단어를 고유한 정수로 부여하는 작업을 진행해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXkb10z0I4IK"
      },
      "source": [
        "encoded = []\n",
        "for line in tokenized: #입력 데이터에서 1줄씩 문장을 읽음\n",
        "    temp = []\n",
        "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
        "      try:\n",
        "        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
        "      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
        "        temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
        "\n",
        "    encoded.append(temp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbSML1B9I6pb"
      },
      "source": [
        "print(encoded[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX9PTu1XI8X1"
      },
      "source": [
        "# 4. 길이가 다른 문장들을 모두 동일한 길이로 바꿔주는 패딩(padding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc8XsEgsI-hy"
      },
      "source": [
        "이제 길이가 다른 리뷰들을 모두 동일한 길이로 바꿔주는 패딩 작업을 진행해보겠습니다. 앞서 단어 집합에 패딩을 위한 토큰인 'pad'를 추가했었습니다. 패딩 작업은 정해준 길이로 모든 샘플들의 길이를 맞춰주되, 길이가 정해준 길이보다 짧은 샘플들에는 'pad' 토큰을 추가하여 길이를 맞춰주는 작업입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0ppss8oJA-D"
      },
      "source": [
        "max_len = max(len(l) for l in encoded)\n",
        "print('리뷰의 최대 길이 : %d' % max_len)\n",
        "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
        "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
        "plt.hist([len(s) for s in encoded], bins=50)\n",
        "plt.xlabel('length of sample')\n",
        "plt.ylabel('number of sample')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e56E4biJCnZ"
      },
      "source": [
        "가장 길이가 긴 리뷰의 길이는 63입니다. 모든 리뷰의 길이를 63으로 통일시켜주겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG2CNZOLJGjx"
      },
      "source": [
        "for line in encoded:\n",
        "    #print(line)\n",
        "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
        "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRa-NUauJIhy"
      },
      "source": [
        "print('리뷰의 최대 길이 : %d' % max(len(l) for l in encoded))\n",
        "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
        "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrINKy_gJKKj"
      },
      "source": [
        "지면의 한계로 인해 상위 3개의 샘플들만 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxRkxlKwJME8"
      },
      "source": [
        "print(encoded[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju1w4cEf8NTN"
      },
      "source": [
        "\n",
        "\n",
        "test_encode =[[1,2,3,4,5],[2,2],[44,45,34]]\n",
        "\n",
        "for line in test_encode:\n",
        "    line += [0]*5\n",
        "\n",
        "\n",
        "print(test_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nILPslaJnWX"
      },
      "source": [
        "이제 단어들을 고유한 정수로 맵핑하였으니, 각 정수를 고유한 단어 벡터로 바꾸는 작업이 필요합니다. 단어 벡터를 얻는 방법은 크게 원-핫 인코딩과 워드 임베딩이 있는데, 주로 워드 임베딩이 사용됩니다. 원-핫 인코딩과 워드 임베딩에 대해서는 다음시간에 다뤄 보겠습니다."
      ]
    }
  ]
}